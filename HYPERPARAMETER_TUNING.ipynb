{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d3becee-d269-4aff-88b3-a9a2ef6c4df3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Install and Import Needed Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a51359ea-b9ee-4211-b0c6-0a13818dc5e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting xgboost\n  Downloading xgboost-2.1.0-py3-none-manylinux_2_28_x86_64.whl (153.9 MB)\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.9/site-packages (from xgboost) (1.21.5)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.9/site-packages (from xgboost) (1.7.3)\nCollecting nvidia-nccl-cu12\n  Downloading nvidia_nccl_cu12-2.22.3-py3-none-manylinux2014_x86_64.whl (190.9 MB)\nInstalling collected packages: nvidia-nccl-cu12, xgboost\nSuccessfully installed nvidia-nccl-cu12-2.22.3 xgboost-2.1.0\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "910ad6cc-5501-4b18-96b9-b633bbb997c2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting mlflow\n  Downloading mlflow-2.14.3-py3-none-any.whl (25.8 MB)\nCollecting docker<8,>=4.0.0\n  Downloading docker-7.1.0-py3-none-any.whl (147 kB)\nCollecting gitpython<4,>=3.1.9\n  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\nCollecting graphene<4\n  Downloading graphene-3.3-py2.py3-none-any.whl (128 kB)\nRequirement already satisfied: pyarrow<16,>=4.0.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (7.0.0)\nCollecting pyyaml<7,>=5.1\n  Downloading PyYAML-6.0.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (738 kB)\nCollecting cloudpickle<4\n  Downloading cloudpickle-3.0.0-py3-none-any.whl (20 kB)\nRequirement already satisfied: pytz<2025 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (2021.3)\nRequirement already satisfied: protobuf<5,>=3.12.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (3.19.4)\nRequirement already satisfied: requests<3,>=2.17.3 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (2.27.1)\nRequirement already satisfied: numpy<2 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.21.5)\nCollecting cachetools<6,>=5.0.0\n  Downloading cachetools-5.4.0-py3-none-any.whl (9.5 kB)\nCollecting alembic!=1.10.0,<2\n  Downloading alembic-1.13.2-py3-none-any.whl (232 kB)\nRequirement already satisfied: Jinja2<4,>=2.11 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (2.11.3)\nRequirement already satisfied: scipy<2 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.7.3)\nRequirement already satisfied: matplotlib<4 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (3.5.1)\nRequirement already satisfied: scikit-learn<2 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.0.2)\nCollecting importlib-metadata!=4.7.0,<8,>=3.7.0\n  Downloading importlib_metadata-7.2.1-py3-none-any.whl (25 kB)\nRequirement already satisfied: entrypoints<1 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (0.4)\nCollecting markdown<4,>=3.3\n  Downloading Markdown-3.6-py3-none-any.whl (105 kB)\nCollecting opentelemetry-sdk<3,>=1.9.0\n  Downloading opentelemetry_sdk-1.25.0-py3-none-any.whl (107 kB)\nRequirement already satisfied: pandas<3 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (1.4.2)\nCollecting querystring-parser<2\n  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\nRequirement already satisfied: packaging<25 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (21.3)\nCollecting gunicorn<23\n  Downloading gunicorn-22.0.0-py3-none-any.whl (84 kB)\nCollecting opentelemetry-api<3,>=1.9.0\n  Downloading opentelemetry_api-1.25.0-py3-none-any.whl (59 kB)\nCollecting Flask<4\n  Downloading flask-3.0.3-py3-none-any.whl (101 kB)\nCollecting sqlalchemy<3,>=1.4.0\n  Downloading SQLAlchemy-2.0.31-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\nCollecting sqlparse<1,>=0.4.0\n  Downloading sqlparse-0.5.1-py3-none-any.whl (44 kB)\nRequirement already satisfied: click<9,>=7.0 in /databricks/python3/lib/python3.9/site-packages (from mlflow) (8.0.4)\nCollecting Mako\n  Downloading Mako-1.3.5-py3-none-any.whl (78 kB)\nRequirement already satisfied: typing-extensions>=4 in /databricks/python3/lib/python3.9/site-packages (from alembic!=1.10.0,<2->mlflow) (4.1.1)\nRequirement already satisfied: urllib3>=1.26.0 in /databricks/python3/lib/python3.9/site-packages (from docker<8,>=4.0.0->mlflow) (1.26.9)\nCollecting Werkzeug>=3.0.0\n  Downloading werkzeug-3.0.3-py3-none-any.whl (227 kB)\nCollecting click<9,>=7.0\n  Downloading click-8.1.7-py3-none-any.whl (97 kB)\nCollecting blinker>=1.6.2\n  Downloading blinker-1.8.2-py3-none-any.whl (9.5 kB)\nCollecting Jinja2<4,>=2.11\n  Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\nCollecting itsdangerous>=2.1.2\n  Downloading itsdangerous-2.2.0-py3-none-any.whl (16 kB)\nCollecting gitdb<5,>=4.0.1\n  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\nCollecting smmap<6,>=3.0.1\n  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\nCollecting aniso8601<10,>=8\n  Downloading aniso8601-9.0.1-py2.py3-none-any.whl (52 kB)\nCollecting graphql-relay<3.3,>=3.1\n  Downloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\nCollecting graphql-core<3.3,>=3.1\n  Downloading graphql_core-3.2.3-py3-none-any.whl (202 kB)\nCollecting zipp>=0.5\n  Downloading zipp-3.19.2-py3-none-any.whl (9.0 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /databricks/python3/lib/python3.9/site-packages (from Jinja2<4,>=2.11->mlflow) (2.0.1)\nRequirement already satisfied: python-dateutil>=2.7 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (2.8.2)\nRequirement already satisfied: fonttools>=4.22.0 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (4.25.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (1.3.2)\nRequirement already satisfied: pyparsing>=2.2.1 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (3.0.4)\nRequirement already satisfied: cycler>=0.10 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (0.11.0)\nRequirement already satisfied: pillow>=6.2.0 in /databricks/python3/lib/python3.9/site-packages (from matplotlib<4->mlflow) (9.0.1)\nCollecting deprecated>=1.2.6\n  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\nCollecting importlib-metadata!=4.7.0,<8,>=3.7.0\n  Downloading importlib_metadata-7.1.0-py3-none-any.whl (24 kB)\nCollecting wrapt<2,>=1.10\n  Downloading wrapt-1.16.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\nCollecting opentelemetry-semantic-conventions==0.46b0\n  Downloading opentelemetry_semantic_conventions-0.46b0-py3-none-any.whl (130 kB)\nRequirement already satisfied: six>=1.5 in /databricks/python3/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib<4->mlflow) (1.16.0)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow) (3.3)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow) (2.0.4)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests<3,>=2.17.3->mlflow) (2021.10.8)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn<2->mlflow) (2.2.0)\nRequirement already satisfied: joblib>=0.11 in /databricks/python3/lib/python3.9/site-packages (from scikit-learn<2->mlflow) (1.1.1)\nCollecting typing-extensions>=4\n  Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\nCollecting greenlet!=0.4.17\n  Downloading greenlet-3.0.3-cp39-cp39-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (614 kB)\nCollecting MarkupSafe>=2.0\n  Downloading MarkupSafe-2.1.5-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\nInstalling collected packages: zipp, wrapt, importlib-metadata, deprecated, typing-extensions, smmap, opentelemetry-api, MarkupSafe, greenlet, graphql-core, Werkzeug, sqlalchemy, opentelemetry-semantic-conventions, Mako, Jinja2, itsdangerous, graphql-relay, gitdb, click, blinker, aniso8601, sqlparse, querystring-parser, pyyaml, opentelemetry-sdk, markdown, gunicorn, graphene, gitpython, Flask, docker, cloudpickle, cachetools, alembic, mlflow\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing-extensions 4.1.1\n    Not uninstalling typing-extensions at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-2091d22e-9554-4bff-a0c8-b3ccd88c1704\n    Can't uninstall 'typing-extensions'. No files were found to uninstall.\n  Attempting uninstall: MarkupSafe\n    Found existing installation: MarkupSafe 2.0.1\n    Not uninstalling markupsafe at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-2091d22e-9554-4bff-a0c8-b3ccd88c1704\n    Can't uninstall 'MarkupSafe'. No files were found to uninstall.\n  Attempting uninstall: Jinja2\n    Found existing installation: Jinja2 2.11.3\n    Not uninstalling jinja2 at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-2091d22e-9554-4bff-a0c8-b3ccd88c1704\n    Can't uninstall 'Jinja2'. No files were found to uninstall.\n  Attempting uninstall: click\n    Found existing installation: click 8.0.4\n    Not uninstalling click at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-2091d22e-9554-4bff-a0c8-b3ccd88c1704\n    Can't uninstall 'click'. No files were found to uninstall.\nSuccessfully installed Flask-3.0.3 Jinja2-3.1.4 Mako-1.3.5 MarkupSafe-2.1.5 Werkzeug-3.0.3 alembic-1.13.2 aniso8601-9.0.1 blinker-1.8.2 cachetools-5.4.0 click-8.1.7 cloudpickle-3.0.0 deprecated-1.2.14 docker-7.1.0 gitdb-4.0.11 gitpython-3.1.43 graphene-3.3 graphql-core-3.2.3 graphql-relay-3.2.0 greenlet-3.0.3 gunicorn-22.0.0 importlib-metadata-7.1.0 itsdangerous-2.2.0 markdown-3.6 mlflow-2.14.3 opentelemetry-api-1.25.0 opentelemetry-sdk-1.25.0 opentelemetry-semantic-conventions-0.46b0 pyyaml-6.0.1 querystring-parser-1.2.4 smmap-5.0.1 sqlalchemy-2.0.31 sqlparse-0.5.1 typing-extensions-4.12.2 wrapt-1.16.0 zipp-3.19.2\nPython interpreter will be restarted.\nPython interpreter will be restarted.\nCollecting hyperopt\n  Downloading hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\nRequirement already satisfied: numpy in /databricks/python3/lib/python3.9/site-packages (from hyperopt) (1.21.5)\nRequirement already satisfied: scipy in /databricks/python3/lib/python3.9/site-packages (from hyperopt) (1.7.3)\nRequirement already satisfied: cloudpickle in /local_disk0/.ephemeral_nfs/envs/pythonEnv-2091d22e-9554-4bff-a0c8-b3ccd88c1704/lib/python3.9/site-packages (from hyperopt) (3.0.0)\nCollecting networkx>=2.2\n  Downloading networkx-3.2.1-py3-none-any.whl (1.6 MB)\nCollecting future\n  Downloading future-1.0.0-py3-none-any.whl (491 kB)\nCollecting py4j\n  Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\nCollecting tqdm\n  Downloading tqdm-4.66.4-py3-none-any.whl (78 kB)\nRequirement already satisfied: six in /databricks/python3/lib/python3.9/site-packages (from hyperopt) (1.16.0)\nInstalling collected packages: tqdm, py4j, networkx, future, hyperopt\nSuccessfully installed future-1.0.0 hyperopt-0.2.7 networkx-3.2.1 py4j-0.10.9.7 tqdm-4.66.4\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install mlflow\n",
    "%pip install hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86e34075-7f27-42bb-9215-59611bce5828",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/databricks/python/lib/python3.9/site-packages/sklearn/experimental/enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    " \n",
    "from xgboost import XGBClassifier\n",
    " \n",
    "from sklearn.experimental import enable_hist_gradient_boosting \n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    " \n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    " \n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\n",
    " \n",
    "from hyperopt import hp, fmin, tpe, SparkTrials, STATUS_OK, space_eval\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.pyfunc\n",
    " \n",
    "import numpy as np\n",
    " \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3643978-5d7c-430f-baba-d7a9e82ecc61",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdd1638c-60ca-4582-8644-23d327d48329",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Set up mlflow experiment in the user's personal workspace folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd73ee97-c854-42ec-acb1-55b97ff587e1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[3]: <Experiment: artifact_location='dbfs:/databricks/mlflow-tracking/2884970239900452', creation_time=1721583839558, experiment_id='2884970239900452', last_update_time=1721591995181, lifecycle_stage='active', name='/Users/anas.ouassou@artefact.com/churn', tags={'mlflow.experiment.sourceName': '/Users/anas.ouassou@artefact.com/churn',\n 'mlflow.experimentType': 'MLFLOW_EXPERIMENT',\n 'mlflow.ownerEmail': 'anas.ouassou@artefact.com',\n 'mlflow.ownerId': '7537234187759097',\n 'mlflow.sharedViewState.d195e7ebd92de2abb1dfcf380da12ad94853b9fb38fcb729aa57017f3cc8679a': '{\"searchFilter\":\"\",\"orderByKey\":\"attributes.start_time\",\"orderByAsc\":false,\"startTime\":\"ALL\",\"lifecycleFilter\":\"Active\",\"datasetsFilter\":[],\"modelVersionFilter\":\"All '\n                                                                                            'Runs\",\"selectedColumns\":[\"attributes.`Source`\",\"attributes.`Models`\",\"attributes.`Dataset`\"],\"runsExpanded\":{},\"runsPinned\":[],\"runsHidden\":[],\"runsHiddenMode\":\"FIRST_10_RUNS\",\"compareRunCharts\":[{\"type\":\"BAR\",\"runsCountToCompare\":10,\"metricSectionId\":\"1721587432862223f0694\",\"deleted\":false,\"isGenerated\":true,\"uuid\":\"1721587432863f854fh66\",\"metricKey\":\"avg '\n                                                                                            'precision\"}],\"compareRunSections\":[{\"uuid\":\"1721587432862223f0694\",\"name\":\"Model '\n                                                                                            'metrics\",\"display\":true,\"isReordered\":false,\"deleted\":false,\"isGenerated\":true},{\"uuid\":\"1721587432862lvupj8qu\",\"name\":\"System '\n                                                                                            'metrics\",\"display\":true,\"isReordered\":false,\"deleted\":false,\"isGenerated\":true}],\"viewMaximized\":false,\"runListHidden\":false,\"isAccordionReordered\":false,\"useGroupedValuesInCharts\":true,\"hideEmptyCharts\":true,\"groupBy\":null,\"groupsExpanded\":{},\"autoRefreshEnabled\":false,\"globalLineChartConfig\":{\"xAxisKey\":\"step\",\"lineSmoothness\":0,\"selectedXAxisMetricKey\":\"\"}}'}>"
     ]
    }
   ],
   "source": [
    "useremail = dbutils.notebook.entry_point.getDbutils().notebook().getContext().userName().get()\n",
    "experiment_name = f\"/Users/{useremail}/churn\"\n",
    "mlflow.set_experiment(experiment_name) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d0e5fdf-446c-40b3-87da-59a64bd101ff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Data Preparation based on EDA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41e9e10a-a5de-492d-8224-64f02a20b7d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13b6be55-45f2-4dbe-a4e9-190e25706814",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def datapreparation(df):\n",
    "    # Define the new column names\n",
    "    new_column_names = [\n",
    "        \"customerID\", \"gender\", \"SeniorCitizen\", \"Partner\", \"Dependents\",\n",
    "        \"tenure\", \"PhoneService\", \"MultipleLines\", \"InternetService\", \n",
    "        \"OnlineSecurity\", \"OnlineBackup\", \"DeviceProtection\", \"TechSupport\",\n",
    "        \"StreamingTV\", \"StreamingMovies\", \"Contract\", \"PaperlessBilling\",\n",
    "        \"PaymentMethod\", \"MonthlyCharges\", \"TotalCharges\", \"Churn\"\n",
    "    ]\n",
    "\n",
    "    # Rename the columns\n",
    "    df.columns = new_column_names\n",
    "\n",
    "    # Drop the first row\n",
    "    df = df.drop(index=0)\n",
    "    df.drop([\"customerID\"], inplace=True, axis=1)\n",
    "    \n",
    "    df.TotalCharges = df.TotalCharges.replace(\" \", np.nan)\n",
    "    df.TotalCharges.fillna(0, inplace=True)\n",
    "    df.TotalCharges = df.TotalCharges.astype(float)\n",
    "    \n",
    "    cols1 = ['Partner', 'Dependents', 'PaperlessBilling', 'Churn', 'PhoneService']\n",
    "    for col in cols1:\n",
    "        df[col] = df[col].apply(lambda x: 0 if x == \"No\" else 1)\n",
    "    \n",
    "    df.gender = df.gender.apply(lambda x: 0 if x == \"Male\" else 1)\n",
    "    df.MultipleLines = df.MultipleLines.map({'No phone service': 0, 'No': 0, 'Yes': 1})\n",
    "    \n",
    "    cols2 = ['OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies']\n",
    "    for col in cols2:\n",
    "        df[col] = df[col].map({'No internet service': 0, 'No': 0, 'Yes': 1})\n",
    "    \n",
    "    df = pd.get_dummies(df, columns=['InternetService', 'Contract', 'PaymentMethod'], drop_first=True)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4eddc0bc-9bb9-45f0-8185-aa99bd79dd2c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Import Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2342212e-7097-41ef-a06a-9eed41b75481",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-2884970239900495>:2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Create a Spark DataFrame from the table\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m df_spark \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mtable(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWA_Fn_UseC__Telco_Customer_Churn_csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Convert the Spark DataFrame to a Pandas DataFrame\u001B[39;00m\n",
       "\u001B[1;32m      5\u001B[0m df \u001B[38;5;241m=\u001B[39m df_spark\u001B[38;5;241m.\u001B[39mtoPandas()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:473\u001B[0m, in \u001B[0;36mDataFrameReader.table\u001B[0;34m(self, tableName)\u001B[0m\n",
       "\u001B[1;32m    439\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtable\u001B[39m(\u001B[38;5;28mself\u001B[39m, tableName: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
       "\u001B[1;32m    440\u001B[0m     \u001B[38;5;124;03m\"\"\"Returns the specified table as a :class:`DataFrame`.\u001B[39;00m\n",
       "\u001B[1;32m    441\u001B[0m \n",
       "\u001B[1;32m    442\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.4.0\u001B[39;00m\n",
       "\u001B[0;32m   (...)\u001B[0m\n",
       "\u001B[1;32m    471\u001B[0m \u001B[38;5;124;03m    >>> _ = spark.sql(\"DROP TABLE tblA\")\u001B[39;00m\n",
       "\u001B[1;32m    472\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n",
       "\u001B[0;32m--> 473\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtable\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtableName\u001B[49m\u001B[43m)\u001B[49m)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `WA_Fn_UseC__Telco_Customer_Churn_csv` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
       "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n",
       "To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.;\n",
       "'UnresolvedRelation [WA_Fn_UseC__Telco_Customer_Churn_csv], [], false\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-2884970239900495>:2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Create a Spark DataFrame from the table\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m df_spark \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mread\u001B[38;5;241m.\u001B[39mtable(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWA_Fn_UseC__Telco_Customer_Churn_csv\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# Convert the Spark DataFrame to a Pandas DataFrame\u001B[39;00m\n\u001B[1;32m      5\u001B[0m df \u001B[38;5;241m=\u001B[39m df_spark\u001B[38;5;241m.\u001B[39mtoPandas()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:473\u001B[0m, in \u001B[0;36mDataFrameReader.table\u001B[0;34m(self, tableName)\u001B[0m\n\u001B[1;32m    439\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtable\u001B[39m(\u001B[38;5;28mself\u001B[39m, tableName: \u001B[38;5;28mstr\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    440\u001B[0m     \u001B[38;5;124;03m\"\"\"Returns the specified table as a :class:`DataFrame`.\u001B[39;00m\n\u001B[1;32m    441\u001B[0m \n\u001B[1;32m    442\u001B[0m \u001B[38;5;124;03m    .. versionadded:: 1.4.0\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    471\u001B[0m \u001B[38;5;124;03m    >>> _ = spark.sql(\"DROP TABLE tblA\")\u001B[39;00m\n\u001B[1;32m    472\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 473\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_df(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jreader\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtable\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtableName\u001B[49m\u001B[43m)\u001B[49m)\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `WA_Fn_UseC__Telco_Customer_Churn_csv` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.;\n'UnresolvedRelation [WA_Fn_UseC__Telco_Customer_Churn_csv], [], false\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: [TABLE_OR_VIEW_NOT_FOUND] The table or view `WA_Fn_UseC__Telco_Customer_Churn_csv` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.;\n'UnresolvedRelation [WA_Fn_UseC__Telco_Customer_Churn_csv], [], false\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a Spark DataFrame from the table\n",
    "df_spark = spark.read.table(\"WA_Fn_UseC__Telco_Customer_Churn_csv\")\n",
    "\n",
    "# Convert the Spark DataFrame to a Pandas DataFrame\n",
    "df = df_spark.toPandas()\n",
    "\n",
    "df = datapreparation(df)\n",
    "\n",
    "# Display the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59a08f7c-7305-42b5-991c-f45614d3bfb9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Some Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7c73df2-08c4-40be-9214-4d5e2d897167",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ensure that SeniorCitizen, tenure, and MonthlyCharges are numeric\n",
    "df['SeniorCitizen'] = pd.to_numeric(df['SeniorCitizen'], errors='coerce')\n",
    "df['tenure'] = pd.to_numeric(df['tenure'], errors='coerce')\n",
    "df['MonthlyCharges'] = pd.to_numeric(df['MonthlyCharges'], errors='coerce')\n",
    "\n",
    "X = df.drop('Churn', axis=1)\n",
    "y = df['Churn']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ensure all columns in X_train and X_test are numeric\n",
    "X_train = X_train.apply(pd.to_numeric, errors='coerce')\n",
    "X_test = X_test.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Check the data types\n",
    "print(X_train.dtypes)\n",
    "print(X_test.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a886f3cc-fab1-446e-b83f-b8821105d5aa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "add63100-3180-4dd9-8a63-c91b8f88c073",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ensure that SeniorCitizen, tenure, and MonthlyCharges are numeric\n",
    "df['SeniorCitizen'] = pd.to_numeric(df['SeniorCitizen'], errors='coerce')\n",
    "df['tenure'] = pd.to_numeric(df['tenure'], errors='coerce')\n",
    "df['MonthlyCharges'] = pd.to_numeric(df['MonthlyCharges'], errors='coerce')\n",
    "\n",
    "X = df.drop('Churn', axis=1)\n",
    "y = df['Churn']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Ensure all columns in X_train and X_test are numeric\n",
    "X_train = X_train.apply(pd.to_numeric, errors='coerce')\n",
    "X_test = X_test.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Check the data types\n",
    "print(X_train.dtypes)\n",
    "print(X_test.dtypes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28c15ea9-5c9a-43a7-9fda-62feda988efa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Define weights for the XGBoost model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d391a4d-c697-449b-8bfd-ce467977d977",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "weights = compute_class_weight(\n",
    "  'balanced', \n",
    "  classes=np.unique(y_train), \n",
    "  y=y_train\n",
    "  )\n",
    " \n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "226737ea-d691-45a8-a645-36c3b74550dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def evaluate_model(hyperopt_params):\n",
    "  \n",
    "  # accesss replicated input data\n",
    "  X_train_input = X_train_broadcast.value\n",
    "  y_train_input = y_train_broadcast.value\n",
    "  X_test_input = X_test_broadcast.value\n",
    "  y_test_input = y_test_broadcast.value  \n",
    "  \n",
    "  # configure model parameters\n",
    "  params = hyperopt_params\n",
    "  \n",
    "  if 'max_depth' in params: params['max_depth']=int(params['max_depth'])   # hyperopt supplies values as float but must be int\n",
    "  if 'min_child_weight' in params: params['min_child_weight']=int(params['min_child_weight']) # hyperopt supplies values as float but must be int\n",
    "  if 'max_delta_step' in params: params['max_delta_step']=int(params['max_delta_step']) # hyperopt supplies values as float but must be int\n",
    "  # all other hyperparameters are taken as given by hyperopt\n",
    "  \n",
    "  # params['tree_method']='gpu_hist'      # settings for running on GPU\n",
    "  # params['predictor']='gpu_predictor'   # settings for running on GPU\n",
    "  \n",
    "  params['tree_method']='hist'      # settings for running on CPU\n",
    "  params['predictor']='cpu_predictor'   # settings for running on CPU\n",
    "  \n",
    "  # instantiate model with parameters\n",
    "  model = XGBClassifier(**params)\n",
    "  \n",
    "  # train\n",
    "  model.fit(X_train_input, y_train_input)\n",
    "  \n",
    "  # predict\n",
    "  y_prob = model.predict_proba(X_test_input)\n",
    "  \n",
    "  # score\n",
    "  model_ap = average_precision_score(y_test_input, y_prob[:,1])\n",
    "  mlflow.log_metric('avg precision', model_ap)  # record actual metric with mlflow run\n",
    "  \n",
    "  # invert metric for hyperopt\n",
    "  loss = -1 * model_ap  \n",
    "  \n",
    "  # return results\n",
    "  return {'loss': loss, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11eda1da-7be5-4eb3-8c2e-f64009b50851",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train_broadcast = sc.broadcast(X_train)\n",
    "X_test_broadcast = sc.broadcast(X_test)\n",
    "y_train_broadcast = sc.broadcast(y_train)\n",
    "y_test_broadcast = sc.broadcast(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "98a151b6-7e6d-4f95-9fc8-5cb70a787045",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "scale = weights[1]/weights[0]\n",
    " \n",
    "# define hyperopt search space\n",
    "search_space = {\n",
    "    'max_depth' : hp.quniform('max_depth', 1, 30, 1)                                  # depth of trees (preference is for shallow trees or even stumps (max_depth=1))\n",
    "    ,'learning_rate' : hp.loguniform('learning_rate', np.log(0.01), np.log(0.40))     # learning rate for XGBoost\n",
    "    ,'gamma': hp.quniform('gamma', 0.0, 1.0, 0.001)                                   # minimum loss reduction required to make a further partition on a leaf node\n",
    "    ,'min_child_weight' : hp.quniform('min_child_weight', 1, 20, 1)                   # minimum number of instances per node\n",
    "    ,'subsample' : hp.loguniform('subsample', np.log(0.1), np.log(1.0))               # random selection of rows for training,\n",
    "    ,'colsample_bytree' : hp.loguniform('colsample_bytree', np.log(0.1), np.log(1.0)) # proportion of columns to use per tree\n",
    "    ,'colsample_bylevel': hp.loguniform('colsample_bylevel', np.log(0.1), np.log(1.0))# proportion of columns to use per level\n",
    "    ,'colsample_bynode' : hp.loguniform('colsample_bynode', np.log(0.1), np.log(1.0)) # proportion of columns to use per node\n",
    "    ,'scale_pos_weight' : hp.loguniform('scale_pos_weight', np.log(scale), np.log(scale * 10))   # weight to assign positive label to manage imbalance\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab0894c1-250d-4208-b554-6a8307cd1508",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# perform evaluation\n",
    "with mlflow.start_run(run_name='HGB'):\n",
    "  argmin = fmin(\n",
    "    fn=evaluate_model,\n",
    "    space=search_space,\n",
    "    algo=tpe.suggest,  # algorithm controlling how hyperopt navigates the search space\n",
    "    max_evals=250,\n",
    "    # trials=SparkTrials(parallelism=2), # THIS VALUE IS ALIGNED WITH THE NUMBER OF WORKERS IN MY GPU-ENABLED CLUSTER (guidance differs for CPU-based clusters)\n",
    "    verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a03f8b2-47de-4b0a-9244-1d9064dc5616",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# release the broadcast datasets\n",
    "X_train_broadcast.unpersist()\n",
    "X_test_broadcast.unpersist()\n",
    "y_train_broadcast.unpersist()\n",
    "y_test_broadcast.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff7e8b9d-1856-4255-a17c-d713d4a80bd7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    " space_eval(search_space, argmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3111536a-b7e1-4b12-88fb-3a1712791079",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define list to hold run ids for later retrieval\n",
    "run_ids = []\n",
    "\n",
    "# train model with optimal settings \n",
    "with mlflow.start_run(run_name='XGB Final Model') as run:\n",
    "  \n",
    "  # capture run info for later use\n",
    "  run_id = run.info.run_id\n",
    "  run_name = run.data.tags['mlflow.runName']\n",
    "  run_ids += [(run_name, run_id)]\n",
    "   \n",
    "  # configure params\n",
    "  params = space_eval(search_space, argmin)\n",
    "  if 'max_depth' in params: params['max_depth']=int(params['max_depth'])       \n",
    "  if 'min_child_weight' in params: params['min_child_weight']=int(params['min_child_weight'])\n",
    "  if 'max_delta_step' in params: params['max_delta_step']=int(params['max_delta_step'])\n",
    "  if 'scale_pos_weight' in params: params['scale_pos_weight']=int(params['scale_pos_weight'])    \n",
    "  params['tree_method']='hist'        # modified for CPU deployment\n",
    "  params['predictor']='cpu_predictor' # modified for CPU deployment\n",
    "  mlflow.log_params(params)\n",
    "  \n",
    "  # train\n",
    "  model = XGBClassifier(**params)\n",
    "  model.fit(X_train, y_train)\n",
    "  mlflow.sklearn.log_model(model, 'model')  # persist model with mlflow\n",
    "  \n",
    "  # predict\n",
    "  y_prob = model.predict_proba(X_test)\n",
    "  \n",
    "  # score\n",
    "  model_ap = average_precision_score(y_test, y_prob[:,1])\n",
    "  mlflow.log_metric('avg precision', model_ap)\n",
    "  \n",
    "  print('Model logged under run_id \"{0}\" with AP score of {1:.5f}'.format(run_id, model_ap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3601623d-5ddf-4f35-b193-c9c880cfdb08",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(y_prob[:,1])"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "HYPERPARAMETER_TUNING",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
